# ======================================================================================================= #

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

# ======================================================================================================= #

[project]
name = "LoRA-Sec"
version = "1.0.0"
description = "Federated Learning"
license = "Apache-2.0"
authors = [
    { name = "1", email = "1@1.gmail.com" },
    { name = "-", email = "-" },
]
dependencies = [
    "flwr[simulation]==1.12.0",
    "flwr-datasets>=0.3.0",
    "torch==2.4.0",
    "transformers>=4.30.0,<5.0",
    "evaluate>=0.4.0,<1.0",
    "datasets>=2.0.0, <3.0",
    "scikit-learn>=1.3.1, <2.0",
]

# ======================================================================================================= #

[tool.hatch.build.targets.wheel]
packages = ["."]

# ======================================================================================================= #

[tool.flwr.app]
publisher = "flwrlabs"

# ======================================================================================================= #

[tool.flwr.app.components]
serverapp = "LoRA_Sec.server_app:app"
clientapp = "LoRA_Sec.client_app:app"

# ======================================================================================================= #

[tool.flwr.app.config]

# ======================================================================================================= # 
# ===================================== Config of Federated Learning ==================================== #

num-server-rounds = 200

fraction-fit =  0.8
fraction-evaluate = 1
number_of_clients = 20

# ----------------- Orthogonality Regularization ----------------- #

use_ortho_loss = true
lambda_ortho = 0.4

# ======================================================================================================= # 
# ============================================ Dataset and Model ======================================== #

partitioner_type = "Dirichlet"     
partitioner_parameter = 0.5             # 0 if partitioner does not have parameter

dataset_name = "IMDB"                   # "IMDB", "Yelp" , "DBPedia"
num_labels = 2                          #  IMDB=2 ,  Yelp=5 , DBPedia=14, 
number_of_samples = 10000               # 0 or negative => use full dataset

model-name =  "prajjwal1/bert-tiny"  

# ======================================================================================================= # 
# ============================================== Attack Config ========================================== #

attack_type = "RandScaling"             # RandScaling, MinMax, MinSum, Trim, PoisonedFL, FedIMP, Mixture
Malicious-Client-Percentage = 0.4                  

# ------------------------------------------------------------------ #
# ----------------------- For PoisonedFL attack -------------------- #

pfl_c0    = 8.0     # initial c
pfl_beta  = 0.7     # multiplicative decay when test fails
pfl_e     = 50      # window length for the binomial test
pfl_min_c = 0.5     # floor for c
pfl_pval  = 0.01    # significance level for the binomial test

# ------------------------------------------------------------------ #
# ----------------- FedIMP (Parameter-Importance MP) --------------- #

fedimp_importance = "fisher"        # "fisher" | "grad2" (alias)
fedimp_topk_ratio = 0.10            # fraction of targeted coords to emphasize
fedimp_scale = 5.0                  # multiplicative boost on selected coords (gamma)
fedimp_flip_sign = false            # reverse sign on selected coords (stronger untargeted damage)
fedimp_noise_std = 0.0              # small Gaussian noise for stealth
fedimp_match_norm = false           # rescale forged delta to benign L2 norm
fedimp_use_lora_only = true         # focus on LoRA A/B by default
fedimp_include_classifier = false   # optionally include classifier head
fedimp_fisher_batches = 2           # #mini-batches to estimate diagonal Fisher
fedimp_fisher_batchsize = 16        # batch size for Fisher probe

fedimp_num_sims = 6                 # K: how many benign simulations per round
fedimp_sim_batches = 2              # small #batches per simulation (not full epoch)
fedimp_delta_max = 10.0             # δ_max for pre-poison and binary search upper bound
fedimp_tau = 1e-3                   # binary-search tolerance on δ
fedimp_eps = 0.0                    # slack in acceptance inequality
fedimp_prepoison_jump = 0.02        # if (l1 - l0) > this, keep δ_max (pre-poison)
fedimp_share_across_malicious = true      # make all compromised clients upload identical Δw
fedimp_shared_dir = "./.fedimp_shared"    # where to share per-round vectors (local sim)
fedimp_leader_cid = "0"             # who computes (μ,σ,mask) and saves (others load)

# ------------------------------------------------------------------ #
# ---------------------------- Mixture attack ---------------------- #

mixture_choices = "ALL"   # or: "RandScaling, Trim, MinMax, MinSum, PoisonedFL, FedIMP"
mixture_seed = 1337

# ======================================================================================================= # 
# ============================================== Defense Config ========================================= # newFedAvg

strategy = "MultiKrum"              #  FedAvg, Krum, MultiKrum, TrimmedMean, FLTrust, DnC, FoolsGold, FedDMC, ShieldFL, ORLA, ORLA-Flex  

strategy_parameter = 6              # FedAvg: -
                                    # Krum: 0.4          (trim_ratio)
                                    # MultiKrum: 6       ("m" ratio of clients as winners)
                                    # TrimmedMean: 0.2   (Top and bottom trim_ratio) 
                                    # FLTrust: -
                                    # DnC: 0.4           (trim_ratio)  
                                    # FoolsGold: 1       (κ: confidence for the logit)
                                    # FedDMC: -
                                    # ShieldFL: -
                                    # ORLA: 0.4          (trim_ratio)
                                    # ORLA-Flex: 6       (minimum retention count) 

strategy_parameter2 = 6             # MultiKrum: 6       ("f" upper bound on Byzantine clients)

# ------------------------------------------------------------------ #
# ------------------------- For FedDMC Defense --------------------- #

# FedDMC (PCA + BTBCN + SEDC)
feddmc_k = 10                      # PCA dims (paper discusses varying k; lower k -> faster & clearer separation)
feddmc_min_cluster_size = 3        # pruning threshold in BTBCN (paper’s fig. shows small values effective)
feddmc_alpha = 0.7                 # EMA memory for SEDC; higher remembers history more
feddmc_threshold = 0.5             # recommended in paper
feddmc_use_lora_only = true        # align with LoRA-delta setting
feddmc_include_classifier = false

# ======================================================================================================= # 
# ======================================================================================================= # 

[tool.flwr.federations]
default = "local-simulation"

# ======================================================================================================= #

[tool.flwr.federations.local-simulation]
options.num-supernodes = 20  

# ======================================================================================================= #

[tool.flwr.federations.local-simulation-gpu]
options.num-supernodes = 20  
options.backend.client-resources.num-cpus = 4      # each ClientApp assumes to use 4CPUs
options.backend.client-resources.num-gpus = 0.25   # at most 4 ClientApp will run in a given GPU (lower it to increase parallelism)

# ======================================================================================================= #
